## work & span

- informal example: Fibonacci sequence
    + motivate work & span

- work (W): number of primitive operations
    + "time complexity"
    + (relation to time in sequential machines): coincides
    + (relation to time in parallel machines): time = W/P in case of *perfect speedup*

- span (S): |the longest dependency chain|
    + time that would be taken given an infinite number of cores

- intrinsic parallelism (P): W/S
    + dependency matters for parallelism

- W, S, P are all *abstract resources*

- we'll mostly focus on "work-efficient" algorithms
    + work-efficient: an algorithm A is work-efficient for a problem P if A's work is asymptotically the best for P.
    + motivation: energy (~= work) is usually the most important measure.
    + among work-efficient algorithms, try to minimize S

- example: sequential & parallel pair (composition)

- example: Fibonacci sequence
    + W = O(2^n), Theta(Fib(n)) ~= Theta(1.6^n) (by some mathematical analysis)
      S = O(n)
    + (1 1
       1 0) ^n times (1 1)^T
      W = O(lg n)
      S = O(lg n)




# part I: background

- students will read them all by themselves.
- quiz? homework?




# asymptotics

- motivation: ignore constant factors (effectively also ignoring lower-order terms),
              ignore small inputs
    + Q: how to precisely define "ignore ..."?

- asymptotic dominance: f dominates g if exists c,n0 s.t. for all n>= 0, g(n) <= c f(n)
    + e.g. n dominates (ln^k n).  proof: sufficient to prove that lim_(n->inf) (ln^k n)/n = 0
    + pre-order
    + equivalence class: Theta  (e.g. (n+1)^2 \in Theta(n^2))

- O, Omega: upper & lower bound

- notations: "f(n) = O(n)", "O(\lambda n. n+a)", "O(n*m)", ...

- doesn't always reflect the real cost: insertion sort works better than merge sort for small inputs




# example: sorts

- insertion sort
    + work = O(n^2), span = O(n lg n)
- (parallel) merge sort
    + work = O(n lg n), span = O(lg^2 n)
      how to reduce span from O(nz) to O(lg^2 n)? see http://www.classes.cec.wustl.edu/~cse341/web/handouts/lecture10.pdf




# recurrences

- motivation: many algorithms reduce a problem into smaller ones,
              and thus costs are recursively represented.

- examples
    + insertion sort's work: W(n) = ... = W(n-1) + O(n)
    + insertion sort's span: S(n) = ... = S(n-1) + O(lg n)
    + merge sort's work: W(n) = ... (using if-then-else) = 2W(n/2) + O(n)
    + merge sort's span: S(n) = ... (using if-then-else) = S(n/2) + O(lg n)

- usually ignore base case, floor & ceiling

- should remove O-notation in recurrence
  e.g. "W(n) = W(n-1) + O(n)" means "W(n) <= W(n-1) + Cn + C'" for some C, C'

- draw the call tree for making sense of recurrence.
  if you can make a closed form, do so ("tree method").
  e.g. W(n) = 2W(n/2) + Cn + C'

- check if recurrence is root-/leaf-dominated and exploit its property ("brick method")
    + gives you some intuition where is dominating and to optimize: root? leaf?
    + e.g. W(n) = W(n/2) + n is root-dominated.
    + e.g. W(n) = 3W(n/2) + n is leaf-dominated.
    + e.g. W(n) = W(sqrt(n)) + W(n/2) + n is root-dominated.
    + e.g. W(n) = W(n/2) + W(n/3) + 1 is leaf-dominated (O(n^0.788))

- for a mathematically rigorous argument, you need to do induction ("substitute" method").
  e.g. W(n) <= 2W(n/2) + Cn for n>1 and W(n) <= C for n<=1.  prove W(n) <= Kn lg n + K' for some K and K'.
  proof. K = 2C, K' = C.  do induction.

- math intensive course: solving excercises in the textbook (Section 11.4)




# language (slide)

- motivation: clearly representing algorithms

- notable points
    + pure functions (no side-effects): for safety
      (e.g. race condition incurs 2003 Northeast blackout: 50 Million people in North America)
    + first-class functions: for expressibility
    + (recursive) data types: for information hiding

p := x | (p, p) | ...

e :=
| x
| v
| lambda p. e
| e e
|
| e op e
| e, e
| e || e
| if e then e else e
| let (p = e)+ in e end
| (e)
| case e (p => e)+

- cost model (p75, Definition 10.2)
    + motivation: basis for cost analysis (should be easy and relevant)
    + parallelism should be stated as "||"
    + evaluation orders of application: call-by-value




# example: genome sequencing problem

- motivation: learning algorithm design patterns (reduction, brute force, divide and conquer, contraction, ...)

- problem
    + sequence of "AGCT" (figure?)
    + human genome is large: 3 billions
    + can sequence only short strings, biochemically
    + should sequence long strings, computationally

- mathematical modeling
    + shotgun method: duplicate, randomly cut, sequence, reconstruct
    + reduce to Shortest Super-string (understanding real-world problem into mathematical one)
    + caveat: reading error and repetition (cf. telomere)
    + moral: should adequately balance the trade-off between reality and simplicity

- solution (to Shortest Super-string problem)
    + background: super/sub-string, pre/suffix, Kleene star/plus
    + reduction: remove sub-strings
    + observation: start & end positions are strictly ordered
    + modeling: TSP (permutation & removing overlaps)

    + approach 1: brute force
    + apporach 2: greedy (i.e. merging maximally overlapping fragments)
      (constant-factor approximation algorithm of factor 3.5 (proved) or 2 (conjectured))
    + todo: example.

- overcoming caveats
    + repetition -> double-barrel shotgun method (reading end, approximate length, reading end)
    + reading error -> approximation/scoring/probability

- digression: NP (nondeterministic polynomial time)
    + motivation: TSP is known to be "NP". What is NP, exactly?

    + definition of NP:
      "the set of decision problems for which the problem instances, where the answer is "yes", have proofs verifiable in polynomial work."
      "The set of decision problems solvable in polynomial work by a non-deterministic Turing machine."
      todo: prove equivalence.

    + question: how to verify the optimality of a TSP solution in polynomial work?
      answer: TSP is not a decision problem, and thus is not NP.

    + NP-hard: P is NP-hard if every NP problem can be reduced to P (https://en.wikipedia.org/wiki/NP-hardness)
    + NP-complete: NP and NP-hard (https://en.wikipedia.org/wiki/NP-completeness)

    + proving NP-hardness is not usually enough.
      e.g. TSP in transportation business (Coupang, ...)




# specification (todo: should we learn it?)

- motivation: modularity
- functional spec: what an algorithm should do, not how
    + e.g. comparison sort
- cost spec: how much resource an algorithm takes
    + e.g. W = O(n lg n), S = O(lg^2 n)
    + e.g. W = O(n lg n)
- problem: functional/cost spec
    + e.g. implement comparison sort w/ ...
- solution: implementation that solves the problem
    + insertion sort
    + (parallel) merge sort

- abstract data types (ADT): set of interfacing functions
    + functional/cost spec for those functions
      e.g. priority queue
           new: O(1), O(1)
           push: O(lg n), O(lg n)
           pop: O(lg n), O(lg n)
    + ADT problem, implement data structures





(TODO: Part IV, Part V, Part VI, Part VII, Part VIII, Part IX)
