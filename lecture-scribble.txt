## work & span

- informal example: Fibonacci sequence
    + motivate work & span

- work (W): number of primitive operations
    + "time complexity"
    + (relation to time in sequential machines): coincides
    + (relation to time in parallel machines): time = W/P in case of *perfect speedup*

- span (S): |the longest dependency chain|
    + time that would be taken given an infinite number of cores

- intrinsic parallelism (P): W/S
    + dependency matters for parallelism

- W, S, P are all *abstract resources*

- we'll mostly focus on "work-efficient" algorithms
    + work-efficient: an algorithm A is work-efficient for a problem P if A's work is asymptotically the best for P.
    + motivation: energy (~= work) is usually the most important measure.
    + among work-efficient algorithms, try to minimize S

- example: sequential & parallel pair (composition)

- example: Fibonacci sequence
    + W = O(2^n), Theta(Fib(n)) ~= Theta(1.6^n) (by some mathematical analysis)
      S = O(n)
    + (1 1
       1 0) ^n times (1 1)^T
      W = O(lg n)
      S = O(lg n)




# part I: background

- students will read them all by themselves.
- quiz? homework?




# asymptotics

- motivation: ignore constant factors (effectively also ignoring lower-order terms),
              ignore small inputs
    + Q: how to precisely define "ignore ..."?

- asymptotic dominance: f dominates g if exists c,n0 s.t. for all n>= 0, g(n) <= c f(n)
    + e.g. n dominates (ln^k n).  proof: sufficient to prove that lim_(n->inf) (ln^k n)/n = 0
    + pre-order
    + equivalence class: Theta  (e.g. (n+1)^2 \in Theta(n^2))

- O, Omega: upper & lower bound

- notations: "f(n) = O(n)", "O(\lambda n. n+a)", "O(n*m)", ...

- doesn't always reflect the real cost: insertion sort works better than merge sort for small inputs




# example: sorts

- insertion sort
    + work = O(n^2), span = O(n lg n)
- (parallel) merge sort
    + work = O(n lg n), span = O(lg^2 n)




# recurrences

- motivation: many algorithms reduce a problem into smaller ones,
              and thus costs are recursively represented.

- examples
    + insertion sort's work: W(n) = ... = W(n-1) + O(n)
    + insertion sort's span: S(n) = ... = S(n-1) + O(lg n)
    + merge sort's work: W(n) = ... (using if-then-else) = 2W(n/2) + O(n)
    + merge sort's span: S(n) = ... (using if-then-else) = S(n/2) + O(lg n)

- usually ignore base case, floor & ceiling

- should remove O-notation in recurrence
  e.g. "W(n) = W(n-1) + O(n)" means "W(n) <= W(n-1) + Cn + C'" for some C, C'

- draw the call tree for making sense of recurrence.
  if you can make a closed form, do so ("tree method").
  e.g. W(n) = 2W(n/2) + Cn + C'

- check if recurrence is root-/leaf-dominated and exploit its property ("brick method")
    + gives you some intuition where is dominating and to optimize: root? leaf?
    + e.g. W(n) = W(n/2) + n is root-dominated.
    + e.g. W(n) = 3W(n/2) + n is leaf-dominated.
    + e.g. W(n) = W(sqrt(n)) + W(n/2) + n is root-dominated.
    + e.g. W(n) = W(n/2) + W(n/3) + 1 is leaf-dominated (O(n^0.788))

- for a mathematically rigorous argument, you need to do induction ("substitute" method").
  e.g. W(n) <= 2W(n/2) + Cn for n>1 and W(n) <= C for n<=1.  prove W(n) <= Kn lg n + K' for some K and K'.
  proof. K = 2C, K' = C.  do induction.

- math intensive course: solving excercises in the textbook (Section 11.4)




# language

- motivation: clearly representing algorithms

- notable points
    + pure functions (no side-effects): for safety
      (e.g. race condition incurs 2003 Northeast blackout: 50 Million people in North America)
    + first-class functions: for expressibility
    + (recursive) data types: for information hiding

p := x | (p, p) | ...

e :=
| x
| v
| lambda p. e
| e e
|
| e op e
| e, e
| e || e
| if e then e else e
| let (p = e)+ in e end
| (e)
| case e (p => e)+

- cost model (p75, Definition 10.2)
    + parallelism should be stated as "||"
    + evaluation orders of application: call-by-value




# specification

- motivation: modularity
- functional spec: what an algorithm should do, not how
    + e.g. comparison sort
- cost spec: how much resource an algorithm takes
    + e.g. W = O(n lg n), S = O(lg^2 n)
    + e.g. W = O(n lg n)
- problem: functional/cost spec
    + e.g. implement comparison sort w/ ...
- solution: implementation that solves the problem
    + insertion sort
    + (parallel) merge sort

- abstract data types (ADT): set of interfacing functions
    + functional/cost spec for those functions
      e.g. priority queue
           new: O(1), O(1)
           push: O(lg n), O(lg n)
           pop: O(lg n), O(lg n)
    + ADT problem, implement data structures




# genome sequencing problem

+ sequence of "AGCT"
+ human genome is large: 3 billions
+ fragments
+ shotgun method: duplicate, randomly cut, sequence, reconstruct
+ reconstruct: Shortest Super-string (understanding real-world problem into mathematical one)
    * super/sub-string, pre/suffix, Kleene star/plus
    * caveat: repetition (cf. telomere) -> double-barrel shotgun method (reading end, approximate length, reading end)
    + caveat: reading error -> overcome with approximation/scoring/probability

+ solution
    * remove sub-strings
    * start & end positions are strictly ordered
    * permutation & removing overlaps
    * brute force: TSP
    * greedy: merge maximally overlapping fragments

+ NP-??
    * question on NP-??: NP-?? is about "checkable in polynomial time."
      But is it able to check TSP solution's optimality in polynomial time?
    * NP-hard: TODO
    * NP-complete: TODO
    * constant-factor approximation
      e.g. 3.5 (proved), 2 (conjectured)





(TODO: Part IV, Part V, Part VI, Part VII, Part VIII, Part IX)
